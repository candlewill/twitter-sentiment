\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{float}
\usepackage{setspace}
\usepackage[numbers]{natbib}

\onehalfspacing
%\doublespacing
\restylefloat{table}

\makeatletter
\renewcommand\@seccntformat[1]{}
\makeatother

\begin{document}

\thispagestyle{empty}
\begin{center}

\huge

Utilizing Maximum Entropy Classification for Sentiment Analysis in Twitter
Messages

\large
\vspace{2.0cm}
By

\vspace{1.0cm}
Douglas Anderson

0703557

dander01@uoguelph.ca

\vspace{1.5cm}
CIS*4900 - Undergraduate Research Project

July 29th, 2014 %TODO fix this

\vspace{1.5cm}

Supervisor:

Professor Fei Song

\vspace{1.5cm}

Second Reader:

Professor Xining Li

\vspace{1.5cm}

Draft 2 %TODO ditch this

\end{center}

\newpage
%\pagestyle{headings}
\setcounter{page}{1}

\section{Introduction}

Social media allows people to discuss topics and disseminate their opinions
with a near negligible cost. The low barrier of entry to social media allows
everyone to share their opinions as much as they please. The scale of textual
data from social media is enormous. In order to gain insight into the `overall'
opinion on particular topics, the process of determining sentiment must be
automated.

Performing sentiment analysis is becoming a very important field of study for
many businesses. If a company can determine what consumers think of their
products they can take actions such as improve their products or appease an
unsatisfied customer.

\subsection{Topic Area}

Sentiment analysis is simply a classification task that attempts to label
fragments of text that appropriately summarizes the attitude of the text.
Natural Language Processing (NLP) provides a number of method for preforming
sentiment analysis. Many of these methods, such as Naive Bayes and Maximum
Entropy, take a probabilistic approach to performing the task. Another commonly
used method of performing sentiment analysis is the uses of a support vector
machine (SVM).


\subsection{Specific Problem}

The focus of this undergraduate research project has been to preform sentiment
analysis in data from the microbloging website Twitter. All of the messages,
known as Tweets, are constrained in length to 140 characters. This limit forces
many users of Twitter to use abbreviations and contractions. This data presents
some interesting problems to performing good sentiment analysis due to it's
short and informal nature. Twitter makes it extremely easy to publish a Tweet
and has no way to edit a Tweet once published. This results in more spelling
mistakes and inconsistent grammar then other sources of text. Tweets often also
contain links to other websites in the form of URLs as well as taging system
for topics and other users.

\subsection{Task Description}

Organizers of the SemEval-2013 workshop created various sentiment analysis
tasks to be preformed. The task focused on for this project was task-2b:
Message polarity classification. The organizers used the Amazon service
``Mechanical Turk" to get many humans to annotate Tweets as `positive',
`negative' or `neutral'. In tweets with mixed sentiments, the prevailing
sentiment was chosen. Participants were tasked with creating a classifier to
establish labels for Tweets not in the training dataset. The various
submissions of the participants were then evaluated by the average of the
F-positive and the F-negative. The submissions were allowed categorized as
`unconstrained' or `constrained' depending on if they did or did not use
supplemental data, respectively.

The dataset for the task consists of 6225 labelled Tweets. 36.66\% of the
corpus has `positive' label, 14.04\% has a `negative' label, 49.30\% has a
label that conveys neutral sentiment. This neutral label is one of `neutral',
`objective', or `objective-OR-neutral'.

\section{Background}

Quite a bit of research has been done in the field of sentiment analysis
recently. Much of the work has involved data from other contexts such as Movie
reviews \cite{Pang2002}, and blog posts \cite{Melville2009}. The challenges
associated with classifying Twitter data has attracted researchers in the past
few years \cite{Jianfeng2013} \cite{Barbosa2010} \cite{Gokulakrishnan2012}.
Many previous attempts researched make use of Native Bayes, Maximum Entropy,
and SVM classifiers.

\textit{Note: This section needs to be expanded. I am just not sure what else to write about...}

%TODO extend

\section{Approach}

\subsection{Summary of Maximum Entropy}

The Maximum Entropy classification method was focused on due to the limited
duration of the project. This supervised machine learning method calculates the
probability that a given document $d$ belongs to the class $c$.  The Maximum
Entropy approach is summarized below. For a more detailed description, see:
Nigam et al. \cite{Nigam1999}.

The training data is used to establish constraints in the following fashion:

\begin{equation}
    P(c | d) = \frac{1}{Z(d)} \mathrm{exp}(\sum\limits_{i} \lambda _{i,c} F_{i,c}(d,c) )
\end{equation}

where $Z(d)$ is a normalization function, $F_{i,c}$ is a feature function for
feature $f_{i}$ and class $c$, and $\lambda_{i}$ is a feature parameter.

In order to establish reasonable $\lambda_{i}$ for each feature the Improved
Iterative Scaling (IIS) is employed. Given a set of training data
$\mathcal{D}$, IIS trains a model $\Lambda$ by performing a hillclimbing
procedure with the function $l(\Lambda|\mathcal{D})$, where:

\begin{equation}
    l(\Lambda | \mathcal{D}) = \sum\limits_{d\in\mathcal{D}} \sum\limits_{i}
    \lambda_{i} f_{i}(d,c(d)) - \sum\limits_{d\in\mathcal{D}} \mathrm{log}
    \sum\limits_{c} \mathrm{exp} \sum\limits_{i} \lambda_{i} f_{i}(d,c)
\end{equation}

Starting with any initial vector of parameters $\Lambda$, at each iteration
we improve the parameters by setting $\Lambda = \Lambda + \Delta$.


\subsection{Implementation}

The classifier produced for this project is implemented in \textit{python} and
make extensive use of the \textit{nltk} (Natural Language Toolkit) library. As
well the implementation makes use of the \textit{flex} tool to create a lexer
for tokenization. The classifier does not make uses of any supplemental
datasets and therefore should be considered constrained within the context of
the task. The classifier adheres to the following procedure:

\begin{enumerate}

    \item \textbf{Tokenization} - The text is read into memory and separated into
        tokens. Each token represents an atom of text. Token types include
        word, number, user, hashtag, url, emoticon and punctuation.

    \item \textbf{Normalizing} - Each token is simplified to assist in
        classification by reducing the number of tokens.

        \begin{itemize}

            \item \textbf{Words} are coerced into lowercase

            \item \textbf{Numbers} are replaced with "\#NUM"

            \item \textbf{User} are replaced with "@USER"

            \item \textbf{Hashtags} are replaced with "\#OCTOTHORPE"

            \item \textbf{URLs} are replaced with "@URL"

            \item \textbf{Emoticon} are grouped into four different groups
                based on their intended meaning. Each group is then replaced
                with a representation. (e.g. `:)' $\rightarrow$ `EM\_HAPPY' and
                `;-p' $\rightarrow$ `EM\_WINK')

            \item most \textbf{Punctuation} is replaced with "PUNCT" however
                question marks, punctuation marks, and ellipsis are each
                replaced with their own representation.

        \end{itemize}

    \item \textbf{Feature Selection} - The number of tokens retained as
        features is further reduced.
        \begin{itemize}

            \item By \textbf{Removing Stopwords}, words that do not
                discriminate between classes are eliminated. Because of the
                limited size of the dataset only 28 stopwords are used.

            \item Then \textbf{Uncommon words} are removed. Words that do not
                meet a certain document frequency threshold are eliminated.
                Many of these words are proper nouns that, if retained are
                likely to cause the classifier to overfit to the training set.

        \end{itemize}

    \item \textbf{Splitting} - The corpus of documents are random separated
        into three sets: the \textit{training set}, the \textit{validation set},
        and the \textit{testing set}. Respectively, each dataset is 60\%, 20\%,
        and 20\% of the corpus.

    \item \textbf{Training} - The classifier is trained using the
        \textit{training set}.  The classifier uses the \textit{validation set}
        to evaluate the training process and attempt to curtail overfitting.

    \item \textbf{Testing} - The performance of the dataset is evaluated
        against the \textit{testing set}.

\end{enumerate}

\subsection{Avoiding overfitting}

The implementation created for this project used a \textit{validation set} during
training as the primary mechanism to avoid overfitting. While training the
effectiveness of the classifier was evaluated after every iteration, using the
accuracy or F-positive measure. During the training process the following
procedure was followed:

\begin{enumerate}

    \item The classifier was trained for 5 iterations to establish reasonable
        initial weights.

    \item Evaluate the classifier using the validation set. Like the
        \textit{testing set} that will evaluate the classifiers final
        performance, the \textit{validation set} shares no common examples with
        the \textit{training set} and will therefore give a reasonable
        indication of how the classifier will perform with unseen examples.

    \item Repeat the following until performance degrades for a specified
        number of iterations

    \begin{enumerate}

        \item Train the classifier for another iteration.

        \item Evaluate performance with the \textit{validation set}.

    \end{enumerate}

    \item Restore the $\Lambda$ value of the best performing iteration.

\end{enumerate}

\section{Discussion}

\subsection{Evaluation Method}

After the SemEval-2013 workshop the organizers released a paper discussing the
proceedings and the results of the task \cite{Nakov2013}. The paper establishes
baseline in terms of $F$. $F$ can be calculated $F = (F_{pos} + F_{neg}) / 2$
where $F_{pos} = 2 \frac{P_{pos}R_{pos}}{P_{pos} + R_{pos}}$ and $F_{neg}$ is
calculated in a similar fashion. The \textit{Precision} measure for the
positive label is $P_{pos} = \frac{\mathrm{\# of\ true\ postive}}{\mathrm{\#
of\ true\ postive} + \mathrm{\# of\ false\ postive} }$. The \textit{Recall}
measure for the positive label is defined as $R_{pos} = \frac{\mathrm{\# of\
true\ postive}}{\mathrm{\# of\ true\ postive} + \mathrm{\# of\ false\ negative}
}$

\subsection{Baseline}

TODO establish baseline here

\subsection{Results}

Since the \textit{test set} is random subset of the corpus, the results of the
classifier can vary slightly between runs. In order to effectively evaluate the
performance of the classifier under certain conditions each experiment is
conducted 3 times and averaged.

\begin{table}[H]
    \begin{center}
    \begin{tabular}{|c|c|c|c|r|r|r|r|}
        \hline
        Description & Stopwords & Uncommon  & Normalize & Run 1 & Run 2 & Run 3 & $\mu F$\\
        \hline
        MaxEnt      & Yes       & Yes       & Yes       & 50.97 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & Yes       & No        & 56.21 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & No        & Yes       & 51.25 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & No        & No        & 54.44 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & Yes       & Yes       & 50.71 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & Yes       & No        & 51.03 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & No        & Yes       & 50.41 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & No        & No        & 51.56 & 0.0   & 0.0   & 0.0 \\
        Naive Bayes & Yes       & Yes       & Yes       & 50.27 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & Yes       & No        & 50.27 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & No        & Yes       & 50.85 & 0.0   & 0.0   & 0.0 \\
        \dots       & Yes       & No        & No        & 52.12 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & Yes       & Yes       & 47.19 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & Yes       & No        & 50.56 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & No        & Yes       & 49.82 & 0.0   & 0.0   & 0.0 \\
        \dots       & No        & No        & No        & 52.15 & 0.0   & 0.0   & 0.0 \\
        \hline
    \end{tabular}
    \end{center}
    \caption{Results of variations on feature selection and training. Sorted by
        average F-score.}
    \label{table:results}
\end{table}

%TODO write about the number of features

\subsection{Comparison to Workshop Participants}

Table \ref{table:comparison} shows the performance of the classifier produced
for this project compared to results of the workshop participants. The full
table (with out this project's results) can be found in table 9 of Nakov et al.
\cite{Nakov2013}.

\begin{table}[H]
    \begin{center}
    \begin{tabular}{|r|l|}
        \hline
        Team       & F \\
        \hline
        NRC-Canada & 69.02 \\
        GU-MLT-LT  & 65.27 \\
        teragram   & 64.86 \\
        BOUNCE     & 63.53 \\
        KLUE       & 63.06 \\
        AMI\&ERIC  & 62.55 \\
        FBM        & 61.17 \\
        AVAYA      & 60.84 \\
        SAIL       & 60.14 \\
        UT-DB      & 59.87 \\
        FBK-irst   & 59.76 \\
        nlp.cs.aueb.gr & 58.91 \\
        UNITOR     & 58.27 \\
        LVIC-LIMSI & 57.14 \\
        Unigon     & 56.96 \\
        NILC\_USP  & 56.31 \\
        DataMining & 55.52 \\
        ECNUCS     & 55.05 \\
        nlp.cs.aueb.gr & 54.73 \\
        ASVUniOfLeipzig & 54.56 \\
        SZTE-NLP   & 54.33 \\
        CodeX      & 53.89 \\
        Oasis      & 53.84 \\
        \textbf{Best MaxEnt}& \textbf{53.32} \\
        NTNU       & 53.23 \\
        UoM        & 51.81 \\
        SSA-UO     & 50.17 \\
        SenselyticTeam & 50.10 \\
        $\ldots$   & $\ldots$ \\
        9 more teams &  \textless\ 50 \\
        $\ldots$   & $\ldots$ \\
        \hline
        Majority Baseline & 29.19 \\
        \hline
    \end{tabular}
    \end{center}
    \caption{Comparison of constrained classifiers sumitted to the SemEval-2013
        workshop. Sorted by F-score.}
    \label{table:comparison}
\end{table}

\section{Conclusion}

I am great and without reproach.

%TODO get a stronger conclusion

\section{Future Work}

\subsection{Hashtags}

Hashtags are a way that Twitter users tag a Tweet as part of a particular
conversation. Hashtags refer to proper nouns such as events, places, or people.
Hashtags can also denote modifiers (such as `\#sarcasm') to the sentiment of the
Tweet, or even raw sentiment (e.g. `\#yay').

While it is likely infeasible to retain Hashtags about proper nouns,
classification performance could be improved by including modifiers and
Hashtags with raw sentiment. The following Tweet (not from the dataset) is a
great example of a Tweet that Hashtags could help improve classification
accuracy.

\begin{verbatim}
    Well I have an ear infection #good #sarcasm
\end{verbatim}

While a simple approach could be to strip the octothorpe from the front of the
Hashtag (e.g. `\#good' $\rightarrow$ `good'), the semantic meaning of the two
are different. Hashtags often have message level meaning while simple words
have local context withing the sentence. Since the dataset created for the
SemEval-2013 workshop mostly contained examples of Hashtags used as proper
nouns, more annotated Tweets would be needed for training and testing.

As well Hashtags sometimes use CamelCase to create multi-word tags (e.g.
`\#SorryNotSorry'). It could prove worthwhile to attempt the decompose
CamelCase Hashtags into its component words during feature selection.

\subsection{Users}

Nearly all Twitter users have published more than one tweet. A more advanced
classifier could establish a particular users propensity to publishing a
prevailing sentiment in all of their tweets.

As well, some Twitter users may be frequently be referred to with a particular
sentiment. For a hypothetical example, Tweets addressed to Emma Watson
(`@EmWatson') are more likely to have a positive sentiment, while tweets to
Barack Obama (`@BarackObama') may contain extremely mixed sentiment.

A challenge that this method could introduce is the introduction of a temporal
element since people's outlooks, and therefore sentiment's in Tweets, will
change over time. As well as public perception of a individual changes, the
prevailing sentiment my change as well.


% Bibliography
\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{paper}

\end{document}

