\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{float}
\usepackage{setspace}
\usepackage[numbers]{natbib}

\onehalfspacing
%\doublespacing
\restylefloat{table}

\makeatletter
\renewcommand\@seccntformat[1]{}
\makeatother

\begin{document}

\thispagestyle{empty}
\begin{center}

\huge

Utilizing maximum entropy classifiers for sentiment analysis in twitter messages

\large
\vspace{2.0cm}
By

\vspace{1.0cm}
Douglas Anderson

0703557

dander01@uoguelph.ca

\vspace{1.5cm}
CIS*4900 - Undergraduate Research Project

August 1, 2014 %TODO fix this

\vspace{1.5cm}

Supervisor:

Professor Fei Song

\vspace{1.5cm}

Second Reader:

Professor Xining Li

\end{center}


\newpage
\pagestyle{headings}
\setcounter{page}{1}

\section{Introduction}

A great deal of modern communications technologies allow people to discuss and
disseminate their opinions with a near negligible cost. Since the extremely low
cost allows everyone to share their opinions, the scale of textual data is
enormous. In order to gain insight into the `overall' opinion on particular
topics, the process of determining sentiment must be automated.

\subsection{Topic Area}

Sentiment analysis is simply a classification task that attempts to label
fragments of text with a label that appropriately summarizes the attitude of
the text. Natural Language Processing (NLP) provides a number of method for
preforming sentiment analysis. Many of these methods take a probabilistic
approach to performing the task.

%TODO how could this be useful

\subsection{Specific Problem}

The focus of this undergraduate research project has been to preform sentiment
analysis in data from the microbloging website Twitter. This data presents some
interesting problems to performing good sentiment analysis due to it's short
and informal nature. All of the messages, known as Tweets, are constrained in
length to 140 characters. This limit forces many users of Twitter to use
abbreviations and contractions. Twitter makes it extremely easy to publish a
Tweet and has no way to edit a Tweet once published. This results more spelling
and grammar mistakes then other sources of text. Tweets often also contain
links to other websites in the form of URLs as well as taging system for topics
and other users.

\subsection{Task Description}

Organizers of the SemEval-2013 workshop created various sentiment analysis
tasks to be preformed. The task focused on for this project was task-2b:
Message polarity classification. The organizers used the amazon service
``Mechanical Turk" to get many humans to annotate Tweets as `positive',
`negative' or `neutral'. In tweets with mixed sentiments the prevailing
sentiment was chosen. Participants were tasked with creating a classifier to
establish labels for Tweets not in the training dataset. The various
submissions of the participants were then evaluated by the average of the
F-positive and the F-negative.

\section{Background}

Quite a bit of research has been done in the field of sentiment analysis
recently. Much of the work has involved data from other contexts such as Movie
reviews \cite{Pang2002}, and blog posts \cite{Melville2009}. The challenges
associated with classifying Twitter data has attracted researchers in the past
few years \cite{Jianfeng2013} \cite{Barbosa2010} \cite{Gokulakrishnan2012}.

\section{Approach}

\subsection{Summary of Maximum Entropy}

The Maximum Entropy classification method that was focused on due to the
limited duration of the project. This supervised machine learning method
calculates the probability that a given document $d$ belongs to the class $c$. The maximum entropy approach is well described in \cite{Nigam1999} and is summarized below.

The training data is used to establish constraints in the following fashion.

\begin{equation}
    P(c | d) = \frac{1}{Z(d)} \mathrm{exp}(\sum\limits_{i} \lambda _{i,c} F_{i,c}(d,c) )
\end{equation}

Where $Z(d)$ is a normalization function, $F_{i,c}$ is a feature function for
feature $f_{i}$ and class $c$, and $\lambda_{i}$ is a feature parameter.

In order to establish reasonable $\lambda_{i}$ for each feature the Improved
Iterative Scaling (IIS) is employed. Given a set of training data
$\mathcal{D}$, IIS trains a model $\Lambda$ by performing a hillclimbing procedure with the function $l(\Lambda|\mathcal{D})$, where:

\begin{equation}
    l(\Lambda | \mathcal{D}) = \sum\limits_{d\in\mathcal{D}} \sum\limits_{i}
    \lambda_{i} f_{i}(d,c(d)) - \sum\limits_{d\in\mathcal{D}} \mathrm{log}
    \sum\limits_{c} \mathrm{exp} \sum\limits_{i} \lambda_{i} f_{i}(d,c)
\end{equation}

Starting with any initial vector of parameters $\Lambda$, at each iteration
we improve the parameters by setting $\Lambda = \Lambda + \Delta$.


\subsection{Implementation}

The classifier produced for this project is implemented in \textit{python} and make
extensive use of the \textit{nltk} (Natural Language Toolkit) library.

\begin{enumerate}

    \item \textbf{Tokenization} - The text is read into memory and separated into
        tokens. Each token represents an atom of text. Token types include
        word, number, user, hashtag, url, emoticon and punctuation.

    \item \textbf{Normalizing} - Each token is simplified to assist in
        classification by reducing the number of tokens.

        \begin{itemize}

            \item \textbf{Words} are coerced into lowercase

            \item \textbf{Numbers} are replaced with "\#NUM"

            \item \textbf{User} are replaced with "@USER"

            \item \textbf{Hashtags} are replaced with "\#OCTOTHORPE"

            \item \textbf{URLs} are replaced with "@URL"

            \item \textbf{Emoticon} are grouped into four different groups
                based on their intended meaning. Each group is then replaced
                with a representation. (e.g. `:)' $\rightarrow$ `EM\_HAPPY' and
                `;-p' $\rightarrow$ `EM\_WINK')

            \item most \textbf{Punctuation} is replaced with "PUNCT" however
                question marks, punctuation marks, and ellipsis are each
                replaced with their own representation.

        \end{itemize}

    \item \textbf{Feature Selection} - The number of tokens retained as
        features is further reduced.
        \begin{itemize}

            \item By \textbf{Removing Stopwords}, words that do not
                discriminate between classes are eliminated. Because of the
                limited size of the dataset only 28 stopwords are used.

            \item Then \textbf{Uncommon words} are removed. Words that do not
                meet a certain document frequency threshold are eliminated.
                Many of these words are proper nouns that, if retained are
                likely to cause the classifier to overfit to the training set.

        \end{itemize}

    \item \textbf{Splitting} - The corpus of documents are random separated
        into three sets: the \textit{training set}, the \textit{validation set},
        and the \textit{testing set}. Respectively, Each dataset is 60\%, 20\%,
        and 20\% of the corpus.

    \item \textbf{Training} - The classifier is trained using the
        \textit{training set}.  The classifier uses the \textit{validation set}
        to evaluate the training process and attempt to curtail overfitting.

    \item \textbf{Testing} - The performance of the dataset is evaluated
        against the \textit{testing set}.

\end{enumerate}

\section{Discussion}

\subsection{Baseline}

\subsection{Avoiding overfitting}

The implementation created for this project used a validation set during
training as the primary mechanism to avoid overfitting. While training the
effectiveness of the classifier was evaluated after every iteration, using the
accuracy or F-positive measure. During the training process the following
procedure was followed:

\begin{enumerate}

    \item The classifier was trained for 5 iterations to establish reasonable
        initial weights.

    \item Evaluate the classifier using the validation set. Like the
        \textit{testing set} that will evaluate the classifiers final
        performance, the \textit{validation set} shares no common examples with
        the \textit{training set} and will therefore give a reasonable
        indication of how the classifier will perform with unseen examples.

    \item Repeat the following until performance degrades for a specified
        number of iterations

    \begin{enumerate}

        \item Train the classifier for another iteration.

        \item Evaluate performance with the \textit{validation set}.

    \end{enumerate}

    \item Restore the $\Lambda$ value of the best performing iteration.

\end{enumerate}

\section{Conclusion}

\section{Future Work}

\subsection{Hashtags}

\subsection{Users}

Nearly all Twitter users have published more than one tweet. A more advanced
classifier could establish a particular users propensity to publishing a
prevailing sentiment in all of their tweets.

As well, some Twitter users may be frequently be referred to with a particular
sentiment. For a hypothetical example, Tweets addresses to Emma Watson
(`@EmWatson') are more likely to have a positive sentiment, while tweets to
Barack Obama (`@BarackObama') may contain extremely mixed sentiment.

% Bibliography

\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{paper}

\end{document}

