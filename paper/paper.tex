\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{float}
\usepackage{setspace}
\usepackage[numbers]{natbib}

\onehalfspacing
%\doublespacing
\restylefloat{table}

\makeatletter
\renewcommand\@seccntformat[1]{}
\makeatother

\begin{document}

\thispagestyle{empty}
\begin{center}

\huge

Utilizing maximum entropy classifiers for sentiment analysis in twitter messages

\large
\vspace{2.0cm}
By

\vspace{1.0cm}
Douglas Anderson

0703557

dander01@uoguelph.ca

\vspace{1.5cm}
CIS*4900 - Undergraduate Research Project

August 1, 2014 %TODO fix this

\vspace{1.5cm}

Supervisor:

Professor Fei Song

\vspace{1.5cm}

Second Reader:

Professor Xining Li

\end{center}


\newpage
\pagestyle{headings}
\setcounter{page}{1}

\section{Introduction}

A great deal of modern communications technologies allow people to discuss and
disseminate their opinions with a near negligible cost. Since the extremely low
cost allows everyone to share their opinions, the scale of textual data is
enormous. In order to gain insight into the `overall' opinion on particular
topics, the process of determining sentiment must be automated.

\subsection{Topic Area}

Sentiment analysis is simply a classification task that attempts to label
fragments of text with a label that appropriately summarizes the attitude of
the text. Natural Language Processing (NLP) provides a number of method for
preforming sentiment analysis. Many of these methods take a probabilistic
approach to performing the task.

%TODO how could this be useful

\subsection{Specific Problem}

The focus of this undergraduate research project has been to preform sentiment
analysis in data from the microbloging website Twitter. This data presents some
interesting problems to performing good sentiment analysis due to it's short
and informal nature. All of the messages, known as Tweets, are constrained in
length to 140 characters. This limit forces many users of Twitter to use
abbreviations and contractions. Twitter makes it extremely easy to publish a
Tweet and has no way to edit a Tweet once published. This results more spelling
and grammar mistakes then other sources of text. Tweets often also contain
links to other websites in the form of URLs as well as taging system for topics
and other users.

\subsection{Task Description}

Organizers of the SemEval-2013 workshop created various sentiment analysis
tasks to be preformed. The task focused on for this project was task-2b:
Message polarity classification. The organizers used the amazon service
``Mechanical Turk" to get many humans to annotate Tweets as `positive',
`negative' or `neutral'. In tweets with mixed sentiments the prevailing
sentiment was chosen. Participants were tasked with creating a classifier to
establish labels for Tweets not in the training dataset. The various
submissions of the participants were then evaluated by the average of the
F-positive and the F-negative.

\section{Background}

Quite a bit of research has been done in the field of sentiment analysis
recently. Much of the work has involved data from other contexts such as Movie
reviews \cite{Pang2002}, and blog posts \cite{Melville2009}. The challenges
associated with classifying Twitter data has attracted researchers in the past
few years \cite{Jianfeng2013} \cite{Barbosa2010} \cite{Gokulakrishnan2012}.

\section{Approach}

\subsection{Summary of Maximum Entropy}

The Maximum Entropy classification method that was focused on due to the
limited duration of the project. This supervised machine learning method
calculates the probability that a given document $d$ belongs to the class $c$.

The maximum entropy approach is well described in \cite{Nigam1999} and is
summarized below. The training data is used to establish constraints in the
following fashion.

\begin{equation}
    P(c | d) = \frac{1}{Z(d)} exp( \sum\limits_{i} \lambda _{i,c} F_{i,c}(d,c) )
\end{equation}

Where $Z(d)$ is a normalization function, $F_{i,c}$ is a feature function for
feature $f_{i}$ and class $c$, and $\lambda_{i}$ is a feature parameter.

In order to establish reasonable $\lambda_{i}$ for each feature the Improved
Iterative Scaling is employed.

%TODO outline the IIS algorithm

\subsection{Implementation}

The classifier produced for this project is implemented in \textit{python} and make
extensive use of the \textit{nltk} (Natural Language Toolkit) library.

\begin{enumerate}

    \item \textbf{Tokenization} - The text is read into memory and separated into
        tokens. Each token represents an atom of text. Token types include
        word, number, user, hashtag, url, emoticon and punctuation.

    \item \textbf{Normalizing} - Each token is simplified to assist in
        classification by reducing the number of tokens.

        \begin{itemize}

            \item \textbf{Words} are coerced into lowercase

            \item \textbf{Numbers} are replaced with "\#NUM"

            \item \textbf{User} are replaced with "@USER"

            \item \textbf{Hashtags} are replaced with "\#OCTOTHORPE"

            \item \textbf{URLs} are replaced with "@URL"

            \item \textbf{Emoticon} are grouped into four different groups
                based on their intended meaning. Each group is then replaced
                with a representation. (e.g. `:)' $\rightarrow$ `EM\_HAPPY' and
                `;-p' $\rightarrow$ `EM\_WINK')

            \item most \textbf{Punctuation} is replaced with "PUNCT" however
                question marks, punctuation marks, and ellipsis are each
                replaced with their own representation.

        \end{itemize}

    \item \textbf{Feature Selection} - The number of tokens retained as
        features is further reduced.
        \begin{itemize}

            \item By \textbf{Removing Stopwords}, words that do not
                discriminate between classes are eliminated. Because of the
                limited size of the dataset only 28 stopwords are used.

            \item Then \textbf{Uncommon words} are removed. Words that do not
                meet a certain document frequency threshold are eliminated.
                Many of these words are proper nouns that, if retained are
                likely to cause the classifier to overfit to the training set

        \end{itemize}

    \item \textbf{Training}
    \item \textbf{Testing}
\end{enumerate}

\section{Discussion}

\section{Conclusion}

\section{Future Work}

\section{Works Cited}

\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{paper}

\end{document}

