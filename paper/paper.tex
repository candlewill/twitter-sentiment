\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{courier}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{float}
\usepackage{setspace}
\usepackage[numbers]{natbib}

\onehalfspacing
%\doublespacing
\restylefloat{table}

\makeatletter
\renewcommand\@seccntformat[1]{}
\makeatother

\begin{document}

\thispagestyle{empty}
\begin{center}

\huge

Utilizing Maximum Entropy classification for sentiment analysis in twitter
messages

\large
\vspace{2.0cm}
By

\vspace{1.0cm}
Douglas Anderson

0703557

dander01@uoguelph.ca

\vspace{1.5cm}
CIS*4900 - Undergraduate Research Project

July 29th, 2014 %TODO fix this


\vspace{1.5cm}

Supervisor:

Professor Fei Song

\vspace{1.5cm}

Second Reader:

Professor Xining Li

\vspace{1.5cm}

Draft 1 %TODO ditch this

\end{center}


\newpage
\pagestyle{headings}
\setcounter{page}{1}

\section{Introduction}

Social Media allows people to discuss topics and disseminate their opinions
with a near negligible cost. Since the cost of using social media is so low it
allows everyone to share their opinions as much as they please. Because of this
the scale of textual data is enormous. In order to gain insight into the
`overall' opinion on particular topics, the process of determining sentiment
must be automated.

\subsection{Topic Area}

Sentiment analysis is simply a classification task that attempts to label
fragments of text with a label that appropriately summarizes the attitude of
the text. Natural Language Processing (NLP) provides a number of method for
preforming sentiment analysis. Many of these methods, such as Naive Bayes and
Maximum Entropy, take a probabilistic approach to performing the task. Another
commonly used method of performing sentiment analysis is the uses of a support
vector machine (SVM).

Performing sentiment analysis is becoming a very important field of study for
many businesses. If a company can determine what consumers think of their
products they can take actions such as improve their products or appease an
unsatisfied customer.

\subsection{Specific Problem}

The focus of this undergraduate research project has been to preform sentiment
analysis in data from the microbloging website Twitter. This data presents some
interesting problems to performing good sentiment analysis due to it's short
and informal nature. All of the messages, known as Tweets, are constrained in
length to 140 characters. This limit forces many users of Twitter to use
abbreviations and contractions. Twitter makes it extremely easy to publish a
Tweet and has no way to edit a Tweet once published. This results more spelling
and grammar mistakes then other sources of text. Tweets often also contain
links to other websites in the form of URLs as well as taging system for topics
and other users.

\subsection{Task Description}

Organizers of the SemEval-2013 workshop created various sentiment analysis
tasks to be preformed. The task focused on for this project was task-2b:
Message polarity classification. The organizers used the amazon service
``Mechanical Turk" to get many humans to annotate Tweets as `positive',
`negative' or `neutral'. In tweets with mixed sentiments the prevailing
sentiment was chosen. Participants were tasked with creating a classifier to
establish labels for Tweets not in the training dataset. The various
submissions of the participants were then evaluated by the average of the
F-positive and the F-negative. The submissions were allowed categorized as
`unconstrained' or `constrained' depending on if they did or did not use
supplemental data, respectively.

\section{Background}

Quite a bit of research has been done in the field of sentiment analysis
recently. Much of the work has involved data from other contexts such as Movie
reviews \cite{Pang2002}, and blog posts \cite{Melville2009}. The challenges
associated with classifying Twitter data has attracted researchers in the past
few years \cite{Jianfeng2013} \cite{Barbosa2010} \cite{Gokulakrishnan2012}.
Many previous attempts researched make use of Native Bayes, Maximum Entropy,
and SVM classifiers.

%TODO extend

\section{Approach}

\subsection{Summary of Maximum Entropy}

The Maximum Entropy classification method that was focused on due to the
limited duration of the project. This supervised machine learning method
calculates the probability that a given document $d$ belongs to the class $c$.
The maximum entropy approach is well described in \cite{Nigam1999} and is
summarized below.

The training data is used to establish constraints in the following fashion.

\begin{equation}
    P(c | d) = \frac{1}{Z(d)} \mathrm{exp}(\sum\limits_{i} \lambda _{i,c} F_{i,c}(d,c) )
\end{equation}

Where $Z(d)$ is a normalization function, $F_{i,c}$ is a feature function for
feature $f_{i}$ and class $c$, and $\lambda_{i}$ is a feature parameter.

In order to establish reasonable $\lambda_{i}$ for each feature the Improved
Iterative Scaling (IIS) is employed. Given a set of training data
$\mathcal{D}$, IIS trains a model $\Lambda$ by performing a hillclimbing
procedure with the function $l(\Lambda|\mathcal{D})$, where:

\begin{equation}
    l(\Lambda | \mathcal{D}) = \sum\limits_{d\in\mathcal{D}} \sum\limits_{i}
    \lambda_{i} f_{i}(d,c(d)) - \sum\limits_{d\in\mathcal{D}} \mathrm{log}
    \sum\limits_{c} \mathrm{exp} \sum\limits_{i} \lambda_{i} f_{i}(d,c)
\end{equation}

Starting with any initial vector of parameters $\Lambda$, at each iteration
we improve the parameters by setting $\Lambda = \Lambda + \Delta$.


\subsection{Implementation}

The classifier produced for this project is implemented in \textit{python} and
make extensive use of the \textit{nltk} (Natural Language Toolkit) library. As
well the implementation makes use of the \textit{flex} tool to create a lexer
for tokenization. The classifier does not make uses of any supplemental
datasets and therefore should be considered constrained within the context of
the task.

\begin{enumerate}

    \item \textbf{Tokenization} - The text is read into memory and separated into
        tokens. Each token represents an atom of text. Token types include
        word, number, user, hashtag, url, emoticon and punctuation.

    \item \textbf{Normalizing} - Each token is simplified to assist in
        classification by reducing the number of tokens.

        \begin{itemize}

            \item \textbf{Words} are coerced into lowercase

            \item \textbf{Numbers} are replaced with "\#NUM"

            \item \textbf{User} are replaced with "@USER"

            \item \textbf{Hashtags} are replaced with "\#OCTOTHORPE"

            \item \textbf{URLs} are replaced with "@URL"

            \item \textbf{Emoticon} are grouped into four different groups
                based on their intended meaning. Each group is then replaced
                with a representation. (e.g. `:)' $\rightarrow$ `EM\_HAPPY' and
                `;-p' $\rightarrow$ `EM\_WINK')

            \item most \textbf{Punctuation} is replaced with "PUNCT" however
                question marks, punctuation marks, and ellipsis are each
                replaced with their own representation.

        \end{itemize}

    \item \textbf{Feature Selection} - The number of tokens retained as
        features is further reduced.
        \begin{itemize}

            \item By \textbf{Removing Stopwords}, words that do not
                discriminate between classes are eliminated. Because of the
                limited size of the dataset only 28 stopwords are used.

            \item Then \textbf{Uncommon words} are removed. Words that do not
                meet a certain document frequency threshold are eliminated.
                Many of these words are proper nouns that, if retained are
                likely to cause the classifier to overfit to the training set.

        \end{itemize}

    \item \textbf{Splitting} - The corpus of documents are random separated
        into three sets: the \textit{training set}, the \textit{validation set},
        and the \textit{testing set}. Respectively, Each dataset is 60\%, 20\%,
        and 20\% of the corpus.

    \item \textbf{Training} - The classifier is trained using the
        \textit{training set}.  The classifier uses the \textit{validation set}
        to evaluate the training process and attempt to curtail overfitting.

    \item \textbf{Testing} - The performance of the dataset is evaluated
        against the \textit{testing set}.

\end{enumerate}

\subsection{Avoiding overfitting}

The implementation created for this project used a validation set during
training as the primary mechanism to avoid overfitting. While training the
effectiveness of the classifier was evaluated after every iteration, using the
accuracy or F-positive measure. During the training process the following
procedure was followed:

\begin{enumerate}

    \item The classifier was trained for 5 iterations to establish reasonable
        initial weights.

    \item Evaluate the classifier using the validation set. Like the
        \textit{testing set} that will evaluate the classifiers final
        performance, the \textit{validation set} shares no common examples with
        the \textit{training set} and will therefore give a reasonable
        indication of how the classifier will perform with unseen examples.

    \item Repeat the following until performance degrades for a specified
        number of iterations

    \begin{enumerate}

        \item Train the classifier for another iteration.

        \item Evaluate performance with the \textit{validation set}.

    \end{enumerate}

    \item Restore the $\Lambda$ value of the best performing iteration.

\end{enumerate}

\section{Discussion}

\subsection{Evaluation Method}

After the SemEval-2013 workshop the organizers released a paper discussing the
proceedings and the results of the task \cite{Nakov2013}. The paper establishes
baseline in terms of $F$. $F$ can be calculated $F = (F_{pos} + F_{neg}) / 2$
where $F_{pos} = 2 \frac{P_{pos}R_{pos}}{P_{pos} + R_{pos}}$ and $F_{neg}$ is
calculated in a similar fashion. The Precision measure for the positive label
is $P_{pos} = \frac{\mathrm{\# of\ true\ postive}}{\mathrm{\# of\ true\
postive} + \mathrm{\# of\ false\ postive} }$. The Recall measure for the
positive label is defined as $R_{pos} = \frac{\mathrm{\# of\ true\
postive}}{\mathrm{\# of\ true\ postive} + \mathrm{\# of\ false\ negative} }$

%TODO note that my solution is unconstrained

\subsection{Baseline}

\subsection{Results}

\subsection{Comparison to Workshop Participants}

Table \ref{table:comparison} shows the performance of the classifier produced
for this project ranks compared to results of the workshop participants. The
full table (with out this project's results) can be found in table 9 of Nakov
et al. \cite{Nakov2013}.

\begin{table}[H]
    \begin{center}
    \begin{tabular}{|r|l|}
        \hline
        Run & F \\
        \hline
        NRC-Canada & 69.02 \\
        GU-MLT-LT  & 65.27 \\
        teragram   & 64.86 \\
        BOUNCE     & 63.53 \\
        KLUE       & 63.06 \\
        \hline
        Majority Baseline & 29.19 \\
        \hline
    \end{tabular}
    \end{center}
    \caption{Comparison of constrained classifiers by F-score.}
    \label{table:comparison}
\end{table}

%TODO finsh table

\section{Conclusion}

I am great and without reproach.

%TODO get a stronger conclusion

\section{Future Work}

\subsection{Hashtags}

Hashtags are a way that Twitter users tag a Tweet as part of a particular
conversation. Hashtags refer to proper nouns such as events, places, or people.
Hashtags can also denote modifiers (such as `\#sarcasm') to the sentiment of the
Tweet, or even raw sentiment (e.g. `\#yay').

While it is likely infeasible to retain Hashtags about proper nouns,
classification performance could be improved by including modifiers and
Hashtags with raw sentiment. The following Tweet (not from the dataset) is a
great example of a Tweet that Hashtags could help improve classification
accuracy.

\begin{verbatim}
    Well I have an ear infection #good #sarcasm
\end{verbatim}

While a simple approach could be to strip the octothorpe from the front of the
Hashtag (e.g. `\#good' $\rightarrow$ `good'), the semantic meaning of the two
are different. Hashtags often have message level meaning while simple words
have local context withing the sentence. Since the dataset created for the
SemEval-2013 workshop mostly contained examples of Hashtags used as proper
nouns, more annotated Tweets would be needed for training and testing.

As well Hashtags sometimes use CamelCase to create multi-word tags (e.g.
`\#SorryNotSorry'). It could prove worthwhile to attempt the decompose
CamelCase Hashtags into its component words during feature selection.

\subsection{Users}

Nearly all Twitter users have published more than one tweet. A more advanced
classifier could establish a particular users propensity to publishing a
prevailing sentiment in all of their tweets.

As well, some Twitter users may be frequently be referred to with a particular
sentiment. For a hypothetical example, Tweets addresses to Emma Watson
(`@EmWatson') are more likely to have a positive sentiment, while tweets to
Barack Obama (`@BarackObama') may contain extremely mixed sentiment.

A challenge that this method could introduce is the introduction of a temporal
element since people's outlooks, and therefore sentiment's in Tweets, will
change over time. As well as public perception of a individual changes, the
prevailing sentiment my change as well.


% Bibliography
\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{paper}

\end{document}

